{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e15216",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # quieter logs\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception as e:\n",
    "    print(\"❌ TensorFlow not importable in this environment.\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Built with CUDA: {getattr(tf.test, 'is_built_with_cuda', lambda: 'n/a')()}\")\n",
    "print(f\"XLA available: {tf.config.optimizer.get_jit()}\\n\")\n",
    "\n",
    "# List devices\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "cpus = tf.config.list_physical_devices(\"CPU\")\n",
    "print(f\"CPUs detected: {[d.name for d in cpus]}\")\n",
    "print(f\"GPUs detected: {[d.name for d in gpus]}\")\n",
    "\n",
    "# Enable memory growth (avoids grabbing all VRAM)\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't set memory growth on {gpu}: {e}\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        cc = details.get(\"compute_capability\", \"unknown\")\n",
    "        print(f\"GPU[0] details: {details.get('device_name','?')} (compute capability: {cc})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "print()\n",
    "\n",
    "def timed_matmul(device=\"/CPU:0\", size=4096):\n",
    "    \"\"\"Time a single large matmul on the given device.\"\"\"\n",
    "    with tf.device(device):\n",
    "        a = tf.random.normal([size, size], dtype=tf.float32)\n",
    "        b = tf.random.normal([size, size], dtype=tf.float32)\n",
    "        # warm-up op placement/compilation\n",
    "        _ = tf.matmul(a, b)\n",
    "        t0 = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        _ = c.numpy()  # materialize\n",
    "        dt = time.time() - t0\n",
    "        print(f\"MatMul {size}x{size} on {device}: {dt:.3f} s\")\n",
    "        return dt\n",
    "\n",
    "# Run tests\n",
    "cpu_time = timed_matmul(\"/CPU:0\", size=2048)\n",
    "\n",
    "gpu_time = None\n",
    "if gpus:\n",
    "    try:\n",
    "        gpu_time = timed_matmul(\"/GPU:0\", size=2048)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Tried to run on GPU but failed:\")\n",
    "        print(e)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "if not gpus:\n",
    "    print(\"❌ No GPUs visible to TensorFlow.\")\n",
    "    print(\"   Tips: ensure NVIDIA drivers + CUDA runtime are installed and \"\n",
    "          \"install a CUDA-enabled TF/PyTorch wheel that matches your CUDA (or use CPU builds).\")\n",
    "else:\n",
    "    print(\"✅ GPU is visible to TensorFlow.\")\n",
    "    if gpu_time is not None:\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
    "        print(f\"   CPU time: {cpu_time:.3f}s | GPU time: {gpu_time:.3f}s | Speedup: {speedup:.2f}×\")\n",
    "    else:\n",
    "        print(\"   But the timed GPU matmul failed; check logs above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
