{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e15216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow version: 2.20.0\n",
      "Built with CUDA: True\n",
      "XLA available: \n",
      "\n",
      "CPUs detected: ['/physical_device:CPU:0']\n",
      "GPUs detected: ['/physical_device:GPU:0']\n",
      "GPU[0] details: NVIDIA GeForce RTX 3080 (compute capability: (8, 6))\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759984430.523780    5861 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatMul 2048x2048 on /CPU:0: 0.020 s\n",
      "MatMul 2048x2048 on /GPU:0: 0.008 s\n",
      "\n",
      "Summary:\n",
      "✅ GPU is visible to TensorFlow.\n",
      "   CPU time: 0.020s | GPU time: 0.008s | Speedup: 2.41×\n"
     ]
    }
   ],
   "source": [
    "import os, time, sys\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # quieter logs\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception as e:\n",
    "    print(\"❌ TensorFlow not importable in this environment.\")\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Built with CUDA: {getattr(tf.test, 'is_built_with_cuda', lambda: 'n/a')()}\")\n",
    "print(f\"XLA available: {tf.config.optimizer.get_jit()}\\n\")\n",
    "\n",
    "# List devices\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "cpus = tf.config.list_physical_devices(\"CPU\")\n",
    "print(f\"CPUs detected: {[d.name for d in cpus]}\")\n",
    "print(f\"GPUs detected: {[d.name for d in gpus]}\")\n",
    "\n",
    "# Enable memory growth (avoids grabbing all VRAM)\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: couldn't set memory growth on {gpu}: {e}\")\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        cc = details.get(\"compute_capability\", \"unknown\")\n",
    "        print(f\"GPU[0] details: {details.get('device_name','?')} (compute capability: {cc})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "print()\n",
    "\n",
    "def timed_matmul(device=\"/CPU:0\", size=4096):\n",
    "    \"\"\"Time a single large matmul on the given device.\"\"\"\n",
    "    with tf.device(device):\n",
    "        a = tf.random.normal([size, size], dtype=tf.float32)\n",
    "        b = tf.random.normal([size, size], dtype=tf.float32)\n",
    "        # warm-up op placement/compilation\n",
    "        _ = tf.matmul(a, b)\n",
    "        t0 = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        _ = c.numpy()  # materialize\n",
    "        dt = time.time() - t0\n",
    "        print(f\"MatMul {size}x{size} on {device}: {dt:.3f} s\")\n",
    "        return dt\n",
    "\n",
    "# Run tests\n",
    "cpu_time = timed_matmul(\"/CPU:0\", size=2048)\n",
    "\n",
    "gpu_time = None\n",
    "if gpus:\n",
    "    try:\n",
    "        gpu_time = timed_matmul(\"/GPU:0\", size=2048)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Tried to run on GPU but failed:\")\n",
    "        print(e)\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "if not gpus:\n",
    "    print(\"❌ No GPUs visible to TensorFlow.\")\n",
    "    print(\"   Tips: ensure NVIDIA drivers + CUDA runtime are installed and \"\n",
    "          \"install a CUDA-enabled TF/PyTorch wheel that matches your CUDA (or use CPU builds).\")\n",
    "else:\n",
    "    print(\"✅ GPU is visible to TensorFlow.\")\n",
    "    if gpu_time is not None:\n",
    "        speedup = cpu_time / gpu_time if gpu_time > 0 else float('inf')\n",
    "        print(f\"   CPU time: {cpu_time:.3f}s | GPU time: {gpu_time:.3f}s | Speedup: {speedup:.2f}×\")\n",
    "    else:\n",
    "        print(\"   But the timed GPU matmul failed; check logs above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
