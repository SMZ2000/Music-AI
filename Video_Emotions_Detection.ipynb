{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c860a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128 cuda: 12.8\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14faf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, VideoMAEForVideoClassification\n",
    "import accelerate\n",
    "import scipy\n",
    "import librosa as lr\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from decord import VideoReader\n",
    "from decord import cpu, gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a9613",
   "metadata": {},
   "source": [
    "# Video emotion detection\n",
    "- training a model to detect the emotion present in a video or scene\n",
    "- the scene may have people in it or maybe it could convey a mood based on the color\n",
    "- I selected VideoMAE because it is very effcient with data meaning that it can be used when i dont have enough video data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e410f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoMAE is loaded\n"
     ]
    }
   ],
   "source": [
    "# Load VideoMAE model\n",
    "MAE_model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", dtype=torch.float16)\n",
    "print(\"VideoMAE is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c10aa",
   "metadata": {},
   "source": [
    "# Preprocessing the Video data for emotion detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ad7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: ['a car is shown', 'a group is dancing', 'a man drives a vehicle through the countryside', 'a man drives down the road in an audi', 'a man driving a car', 'a man is driving a car', 'a man is driving down a road', 'a man is driving in a car as part of a commercial', 'a man is driving', 'a man riding the car speedly in a narrow road', 'a man showing the various features of a car', 'a man silently narrates his experience driving an audi', 'a person is driving his car around curves in the road', 'a person telling about a car', 'guy driving a car down the road', 'man talking about a car while driving', 'the man drives the car', 'the man driving the audi as smooth as possible', 'a man is driving', 'guy driving a car down the road']\n",
      "Video segments: video0.mp4\n"
     ]
    }
   ],
   "source": [
    "# loading MSR-VTT dataset with the 'train_9k' and 'test_1k' splits\n",
    "msrvtt_data_train = load_dataset(\"friedrichor/MSR-VTT\", name=\"train_9k\")\n",
    "msrvtt_data_test = load_dataset(\"friedrichor/MSR-VTT\", name=\"test_1k\")\n",
    "\n",
    "# displaying the caption and video segments of the dataset\n",
    "row = msrvtt_data_train['train'][0]\n",
    "print(\"Caption:\", row['caption'])\n",
    "print(\"Video segments:\", row['video'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0722b67",
   "metadata": {},
   "source": [
    "### Reading in the videos from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527af858",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# created to point to where the videos are stored in my local hugging face cache directory\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cache_item \u001b[38;5;241m=\u001b[39m \u001b[43mmsrvtt_data_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# usually the ZIP\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# this is the cache folder \u001b[39;00m\n\u001b[1;32m      5\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(cache_item)  \u001b[38;5;66;03m# cache folder for this dataset/config\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "# created to point to where the videos are stored in my local hugging face cache directory\n",
    "cache_item = msrvtt_data_train.cache_files[\"test\"][0][\"filename\"]  # usually the ZIP\n",
    "\n",
    "# this is the cache folder \n",
    "base_dir = os.path.dirname(cache_item)  # cache folder for this dataset/config\n",
    "\n",
    "vid_path = row[\"video\"]\n",
    "\n",
    "# making sure the video path is absolute\n",
    "if not os.path.isabs(vid_path):\n",
    "    vid_path = os.path.join(base_dir, vid_path)\n",
    "    \n",
    "### Reading in the videos from the dataset\n",
    "def video_clipping(video_path, fps=2,max_frames=32):\n",
    "    \"\"\"Extract frames from a video file.\"\"\"\n",
    "    \n",
    "    # vr is used create a VideoReader object to read the video file which allows for access to individual frames\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    \n",
    "    # idx is used to create a list of frame indices to be extracted from the video or sampled uniformly across the video's duration\n",
    "    idx = list(range(0, len(vr), max(1, len(vr)//max_frames)))[:max_frames]\n",
    "    \n",
    "    # frames is used to extract the frames from the video at the specified indices and convert them to a numpy array\n",
    "    frames = vr.get_batch(idx).asnumpy()   # (T, H, W, 3) uint8\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Testing the video_clipping function\n",
    "clip = video_clipping(row[\"video\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
