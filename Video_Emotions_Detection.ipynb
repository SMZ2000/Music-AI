{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c860a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128 cuda: 12.8\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14faf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, VideoMAEForVideoClassification\n",
    "import accelerate\n",
    "import scipy\n",
    "import librosa as lr\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from decord import VideoReader\n",
    "from decord import cpu, gpu\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a9613",
   "metadata": {},
   "source": [
    "# Video emotion detection\n",
    "- training a model to detect the emotion present in a video or scene\n",
    "- the scene may have people in it or maybe it could convey a mood based on the color\n",
    "- I selected VideoMAE because it is very effcient with data meaning that it can be used when i dont have enough video data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e410f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoMAE is loaded\n"
     ]
    }
   ],
   "source": [
    "# Load VideoMAE model\n",
    "MAE_model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", dtype=torch.float16)\n",
    "print(\"VideoMAE is loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c10aa",
   "metadata": {},
   "source": [
    "# Preprocessing the Video data for emotion detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49da7ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/vishnutheepb/msrvtt?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.26G/4.26G [01:46<00:00, 43.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/zach/.cache/kagglehub/datasets/vishnutheepb/msrvtt/versions/1\n"
     ]
    }
   ],
   "source": [
    "# using the `kagglehub` library to download the MSR-VTT dataset\n",
    "path = kagglehub.dataset_download(\"vishnutheepb/msrvtt\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ad7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSR-VTT sample: {'video_id': ['video0', 'video1'], 'video': ['video0.mp4', 'video1.mp4'], 'caption': [['a car is shown', 'a group is dancing', 'a man drives a vehicle through the countryside', 'a man drives down the road in an audi', 'a man driving a car', 'a man is driving a car', 'a man is driving down a road', 'a man is driving in a car as part of a commercial', 'a man is driving', 'a man riding the car speedly in a narrow road', 'a man showing the various features of a car', 'a man silently narrates his experience driving an audi', 'a person is driving his car around curves in the road', 'a person telling about a car', 'guy driving a car down the road', 'man talking about a car while driving', 'the man drives the car', 'the man driving the audi as smooth as possible', 'a man is driving', 'guy driving a car down the road'], ['in a kitchen a woman adds different ingredients into the pot and stirs it', 'a woman puts prawns and seasonings into a large pot on a stove', 'in the kitchen a woman makes a dish by adding ingredients mixing and allowing to boil on flame', 'a woman adding ingredients to a pot on the stove and stirring', 'instructions on how to cook a dish of prawns or crayfish are given on screen while the chef prepares the dish', 'a woman is in the kitchen making a recipe in a large pot with many ingredients', 'a woman adds some packets of spices and spoonfuls of tomato sauce to a pot then stirs it and covers the pot', 'a person add ingredients to a pot in a counter than stirs it', 'a person puts items in a pot on the stove in the kitchen', 'a woman cooking food with a metal pan on top of a stove', 'a woman adds different ingredients into a a pot on the stove', 'a woman in a kitchen is cooking a stew in a large pan on her stove', 'a women in a multi-color outfit is cooking a stew type dish in a silver pot', 'a woman adds ingredients to a pot that is simmering on a stove', 'a woman is preparing a seafood stew recipe on a stove demonstrating each step herself while at the same time the easy to read directions', 'in a kitchen a lady preferred crayfish with mixing of curry powder', 'a woman and a bowl spoon mixing dish inside kitchen to prepare to serve to eat displaying on screen', 'cooking the dried smoked prawn in a vessel having boiled water and the lied closed', 'a lady is making dried prawns curry and she added tomato puree and salt in it', 'a woman in a colorful scarf is showing how to make a stew']], 'source': ['MSR-VTT', 'MSR-VTT'], 'category': [9, 16], 'url': ['https://www.youtube.com/watch?v=9lZi22qLlEo', 'https://www.youtube.com/watch?v=w4JM08PDEng'], 'start time': [137.72, 184.33], 'end time': [149.44, 206.89], 'id': [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "# loading MSR-VTT dataset with the 'train_9k' and 'test_1k' splits\n",
    "msrvtt_data_train = load_dataset(\"friedrichor/MSR-VTT\", name=\"train_9k\")\n",
    "\n",
    "# displaying the caption and video segments of the dataset\n",
    "row = msrvtt_data_train['train'][:2]\n",
    "print(\"MSR-VTT sample:\",row)\n",
    "#print(\"Caption:\", row['caption'])\n",
    "#print(\"Video segments:\", row['video'])\n",
    "#print(msrvtt_data_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0722b67",
   "metadata": {},
   "source": [
    "### Reading in the videos from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527af858",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m cache_item \u001b[38;5;241m=\u001b[39m msrvtt_data_train\u001b[38;5;241m.\u001b[39mcache_files[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][:]  \u001b[38;5;66;03m# usually the ZIP\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# this is the cache folder \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_item\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# cache folder for this dataset/config\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vid_path \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# making sure the video path is absolute\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/posixpath.py:152\u001b[0m, in \u001b[0;36mdirname\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdirname\u001b[39m(p):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the directory component of a pathname\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     sep \u001b[38;5;241m=\u001b[39m _get_sep(p)\n\u001b[1;32m    154\u001b[0m     i \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrfind(sep) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# created to point to where the videos are stored in my local hugging face cache directory\n",
    "cache_item = msrvtt_data_train.cache_files[\"train\"][:]  # usually the ZIP\n",
    "\n",
    "# this is the cache folder \n",
    "base_dir = os.path.dirname(cache_item)  # cache folder for this dataset/config\n",
    "\n",
    "vid_path = row[\"video\"]\n",
    "\n",
    "# making sure the video path is absolute\n",
    "if not os.path.isabs(vid_path):\n",
    "    vid_path = os.path.join(base_dir, vid_path)\n",
    "    \n",
    "### Reading in the videos from the dataset\n",
    "def video_clipping(video_path, fps=2,max_frames=32):\n",
    "    \"\"\"Extract frames from a video file.\"\"\"\n",
    "    \n",
    "    # vr is used create a VideoReader object to read the video file which allows for access to individual frames\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    \n",
    "    # idx is used to create a list of frame indices to be extracted from the video or sampled uniformly across the video's duration\n",
    "    idx = list(range(0, len(vr), max(1, len(vr)//max_frames)))[:max_frames]\n",
    "    \n",
    "    # frames is used to extract the frames from the video at the specified indices and convert them to a numpy array\n",
    "    frames = vr.get_batch(idx).asnumpy()   # (T, H, W, 3) uint8\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Testing the video_clipping function\n",
    "clip = video_clipping(row[\"video\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
